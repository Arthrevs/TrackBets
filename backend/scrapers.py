"""
TrackBets Scrapers Module

Data fetching utilities for stock prices, news, social sentiment, and document parsing.
Uses fallback mock data when external services are unavailable.
"""

import os

# Try importing optional dependencies
try:
    import yfinance as yf
    HAS_YFINANCE = True
except ImportError:
    HAS_YFINANCE = False

try:
    from GoogleNews import GoogleNews
    HAS_GOOGLENEWS = True
except ImportError:
    HAS_GOOGLENEWS = False

try:
    from duckduckgo_search import DDGS
    HAS_DDGS = True
except ImportError:
    HAS_DDGS = False


# ============================================================================
# STOCK PRICE DATA
# ============================================================================

def get_stock_price(ticker: str) -> tuple:
    """
    Fetch stock price and history for a given ticker.
    
    Returns:
        tuple: (current_price, history_df or None)
    """
    if HAS_YFINANCE:
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period="1mo")
            if not hist.empty:
                current = round(hist['Close'].iloc[-1], 2)
                return current, hist
        except Exception:
            pass
    
    # Fallback mock data
    mock_prices = {
        "ZOMATO.NS": (145.50, None),
        "TATAMOTORS.NS": (980.10, None),
        "RELIANCE.NS": (14425.80, None),
        "TCS.NS": (3920.50, None),
        "INFY.NS": (1456.30, None),
        "PAYTM.NS": (450.00, None),
    }
    
    ticker_upper = ticker.upper()
    if ticker_upper in mock_prices:
        return mock_prices[ticker_upper]
    
    # Default fallback
    return (1000.00, None)


def fetch_stock_price(ticker: str) -> dict:
    """
    Alternative stock price fetcher returning a dictionary.
    Used by some test files.
    """
    price, history = get_stock_price(ticker)
    return {
        "current": price,
        "history": history,
        "ticker": ticker
    }


# ============================================================================
# NEWS DATA
# ============================================================================

def get_news(ticker: str) -> str:
    """
    Fetch news headlines for a ticker.
    
    Returns:
        str: Concatenated news headlines
    """
    # Clean ticker name for search
    search_term = ticker.replace(".NS", "").replace(".BSE", "")
    
    if HAS_GOOGLENEWS:
        try:
            gn = GoogleNews(lang='en', region='IN')
            gn.search(f"{search_term} stock")
            results = gn.results()[:5]
            if results:
                headlines = [r.get('title', '') for r in results]
                return " | ".join(headlines)
        except Exception:
            pass
    
    if HAS_DDGS:
        try:
            with DDGS() as ddgs:
                results = list(ddgs.news(f"{search_term} stock India", max_results=5))
                if results:
                    headlines = [r.get('title', '') for r in results]
                    return " | ".join(headlines)
        except Exception:
            pass
    
    # Mock fallback
    mock_news = {
        "ZOMATO": "Zomato Q3 results beat estimates | Food delivery giant expands to 500 cities | Analysts upgrade to BUY",
        "TATAMOTORS": "Tata Motors EV sales surge 40% | JLR demand strong in US | New Curvv launch imminent",
        "RELIANCE": "Reliance Jio adds 10M subscribers | Retail expansion continues | Green energy investments",
        "TCS": "TCS wins $500M deal | IT sector outlook cautious | Attrition rates stabilize",
        "PAYTM": "Paytm faces regulatory scrutiny | Payments bank issues | Stock under pressure",
    }
    
    for key in mock_news:
        if key in search_term.upper():
            return mock_news[key]
    
    return f"No recent news found for {search_term}"


def fetch_news_headlines(ticker: str) -> list:
    """
    Alternative news fetcher returning a list.
    Used by some test files.
    """
    news_text = get_news(ticker)
    return news_text.split(" | ")


# ============================================================================
# SOCIAL/REDDIT DATA
# ============================================================================

def get_reddit_posts(ticker: str) -> list:
    """
    Fetch Reddit posts/discussions about a stock.
    
    Returns:
        list: List of post titles/summaries
    """
    search_term = ticker.replace(".NS", "").replace(".BSE", "")
    
    if HAS_DDGS:
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(
                    f"site:reddit.com {search_term} stock india",
                    max_results=5
                ))
                if results:
                    return [r.get('title', r.get('body', ''))[:100] for r in results]
        except Exception:
            pass
    
    # Mock fallback
    mock_posts = {
        "ZOMATO": [
            "Zomato Q3 earnings discussion - profits up 200%!",
            "Is Zomato overvalued at current levels?",
            "Long term bullish on food delivery duopoly",
        ],
        "TATAMOTORS": [
            "Tata Motors EV strategy analysis",
            "JLR turnaround finally happening?",
            "Curvv bookings exceeding expectations",
        ],
        "RELIANCE": [
            "Reliance: The everything company",
            "Jio financial services potential",
            "Green energy pivot discussion",
        ],
    }
    
    for key in mock_posts:
        if key in search_term.upper():
            return mock_posts[key]
    
    return [f"No Reddit discussions found for {search_term}"]


# ============================================================================
# SENTIMENT AGGREGATION
# ============================================================================

def get_aggregated_sentiment(ticker: str) -> dict:
    """
    Aggregate sentiment from multiple sources.
    
    Returns:
        dict: Sentiment data with score and sources
    """
    news = get_news(ticker)
    posts = get_reddit_posts(ticker)
    
    # Simple sentiment scoring based on keywords
    positive_words = ['surge', 'profit', 'growth', 'bullish', 'beat', 'upgrade', 'strong']
    negative_words = ['fall', 'loss', 'bearish', 'miss', 'downgrade', 'weak', 'scrutiny']
    
    text = (news + " " + " ".join(posts)).lower()
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    total = pos_count + neg_count
    if total == 0:
        score = 50
    else:
        score = int((pos_count / total) * 100)
    
    return {
        "score": score,
        "label": "Bullish" if score >= 60 else "Bearish" if score < 40 else "Neutral",
        "sources": {
            "news": news[:100],
            "social": posts[0] if posts else "N/A"
        }
    }


def fetch_aggregated_sentiment(ticker: str) -> dict:
    """Alias for get_aggregated_sentiment."""
    return get_aggregated_sentiment(ticker)


# ============================================================================
# DOCUMENT PARSING (Stubs)
# ============================================================================

def get_pdf_text(pdf_file) -> str:
    """
    Extract text from a PDF file.
    
    Args:
        pdf_file: File object or path
        
    Returns:
        str: Extracted text
    """
    try:
        import PyPDF2
        reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in reader.pages[:5]:  # Limit to first 5 pages
            text += page.extract_text() or ""
        return text[:5000]  # Limit length
    except Exception:
        return "[PDF parsing not available - install PyPDF2]"


def get_youtube_text(video_url: str) -> str:
    """
    Extract transcript from a YouTube video.
    
    Args:
        video_url: YouTube video URL
        
    Returns:
        str: Video transcript or description
    """
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        import re
        
        # Extract video ID
        video_id = None
        patterns = [
            r'v=([^&]+)',
            r'youtu\.be/([^?]+)',
            r'embed/([^?]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, video_url)
            if match:
                video_id = match.group(1)
                break
        
        if video_id:
            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            text = " ".join([t['text'] for t in transcript])
            return text[:5000]
    except Exception:
        pass
    
    return "[YouTube transcript not available]"


def get_twitter_sentiment(query: str) -> dict:
    """
    Get Twitter/X sentiment for a query.
    Note: Requires API access which is limited.
    
    Returns:
        dict: Sentiment data
    """
    # Twitter API is now paid/restricted, return mock
    return {
        "score": 65,
        "label": "Slightly Bullish",
        "sample_tweets": [
            f"${query} looking strong today! ðŸ“ˆ",
            f"Holding {query} for the long term",
        ],
        "note": "Twitter API access limited - using estimates"
    }


# ============================================================================
# LEGACY ALIASES (for backward compatibility)
# ============================================================================

def get_stock_data(ticker: str) -> dict:
    """Legacy alias for fetch_stock_price."""
    return fetch_stock_price(ticker)
